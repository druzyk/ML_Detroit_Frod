{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_datasets():\n",
    "    start_time = time.time()\n",
    "    print(\"------------------------\")\n",
    "    print(\"Загрузка данных началась\")\n",
    "    \n",
    "    #Загружаем датасеты\n",
    "    train = pd.read_csv(r'C:\\datasets\\Сбербанк\\train_2.csv', low_memory=False)\n",
    "    test = pd.read_csv(r'C:\\datasets\\Сбербанк\\test_2.csv', low_memory=False)\n",
    "    \n",
    "    train.set_index('ticket_id', inplace=True)\n",
    "    test.set_index('ticket_id', inplace=True)\n",
    "    \n",
    "    print(\"Загрузка данных закончена за - %s seconds - OK\" % (time.time() - start_time))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_datasets(train, test):\n",
    "    start_time = time.time()\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Предобработка данных началась\")\n",
    "    \n",
    "    #Удалим признаки, которые нам не понадобятся для обучения модели\n",
    "    train = train.drop('Unnamed: 0', axis=1) #Артефакт\n",
    "    test = test.drop('Unnamed: 0', axis=1) # Артефакт\n",
    "    print(\"    Размер train до предобработки: {}\".format(train.shape))\n",
    "    print(\"    Размер test до предобработки: {}\".format(test.shape))\n",
    "    \n",
    "    #Оставим в train признаки, присутствующие в test\n",
    "    columns_outside_test = set(train) - set(test) - {'compliance'}\n",
    "    total_columns = set(train) - columns_outside_test\n",
    "    train = train[total_columns]\n",
    "    \n",
    "    #Удалим неинформативные для модели признаки (объясним почему):\n",
    "    #grafitti_status, non_us_str_code, violation_zip_code удалям, так как по данным признакам нет значений\n",
    "    #Tак как признак с grafitti_status у нас отсутствует в данных, то и нет смысла в признаке clean_up_cost\n",
    "    #violation_description - это описание violation_code\n",
    "    #country - не информативен, 99,99% признаков имеют одно значение\n",
    "    #admin_fee, late_fee, state_fee и fine_amount в сумме являются judgment_amount за минусом скидки\n",
    "    #Адреса, имя инспектора, название компании выписавшей штраф и тд - слишком частные даные, не несут\n",
    "    #общей обобщающей способности\n",
    "    delete_columns = ['grafitti_status', 'non_us_str_code', 'violation_zip_code', 'clean_up_cost', \n",
    "                      'violation_description', 'country', 'admin_fee', 'late_fee', 'state_fee', \n",
    "                      'fine_amount', 'violator_name', 'city', 'zip_code', 'mailing_address_str_name', \n",
    "                      'state', 'inspector_name', 'violation_street_number', 'agency_name', \n",
    "                      'mailing_address_str_number', 'violation_street_name']\n",
    "    train = train.drop(delete_columns, axis=1)\n",
    "    test = test.drop(delete_columns, axis=1)\n",
    "    \n",
    "    #Удаляем события в пропусках целевой переменной\n",
    "    train = train.dropna(subset=['compliance'])\n",
    "    \n",
    "    #-----------------------------------------------------------------\n",
    "    #Удалим из violation_code всё что за спецсимволами, так как это уточнение, а нам необходимы обобщающие признаки\n",
    "    def delete_special_case(data):\n",
    "        data['violation_code'] = data['violation_code'].apply(lambda x: x.split('(')[0])\n",
    "        data['violation_code'] = data['violation_code'].apply(lambda x: x.split('/')[0])\n",
    "        data['violation_code'] = data['violation_code'].apply(lambda x: x.split(' ')[0])\n",
    "        data['violation_code'][data['violation_code'].apply(lambda x: x.find('-')<=0)] = ''\n",
    "    delete_special_case(train)\n",
    "    delete_special_case(test)\n",
    "    \n",
    "    #Cтатьи, по которым меньше 100 правонарущений объеденим в одну категорию - Другие\n",
    "    counts = train['violation_code'].value_counts()\n",
    "    train['violation_code'][train['violation_code'].isin(counts[counts < 100].index)] = 'Other'\n",
    "    test['violation_code'][test['violation_code'].isin(counts[counts < 100].index)] = 'Other'\n",
    "    \n",
    "    #Категоризируем violation_code по частоте его выписывания, для этого используем словарь\n",
    "    violation_code_mean = pd.DataFrame(data=[train.groupby('violation_code')['violation_code'].count().index, \n",
    "                       train.groupby('violation_code')['violation_code'].count()]).T\n",
    "    violation_code_mean[1] = violation_code_mean[1] / len(train)\n",
    "    violation_code_dict = {x[0] : x[1] for x in violation_code_mean.itertuples(index=False)}\n",
    "    \n",
    "    def violation_code_categorizer(row):\n",
    "        try:\n",
    "            revenue = violation_code_dict[row['violation_code']]\n",
    "        except:\n",
    "            revenue = violation_code_dict['Other']\n",
    "        return revenue\n",
    "    \n",
    "    test['violation_code'] = test.apply(violation_code_categorizer, axis=1)\n",
    "    train['violation_code'] = train.apply(violation_code_categorizer, axis=1)\n",
    "    \n",
    "    #-----------------------------------------------------------------\n",
    "    #Предобработаем данные с датами\n",
    "    train.ticket_issued_date = pd.to_datetime(train.ticket_issued_date)\n",
    "    test.ticket_issued_date = pd.to_datetime(test.ticket_issued_date)\n",
    "    train.hearing_date = pd.to_datetime(train.hearing_date)\n",
    "    test.hearing_date = pd.to_datetime(test.hearing_date)\n",
    "    #Введем новый признак, количество дней от штрафа до суда\n",
    "    train['timedelta'] = (train.hearing_date - train.ticket_issued_date).dt.days\n",
    "    test['timedelta'] = (test.hearing_date - test.ticket_issued_date).dt.days\n",
    "    #Теперь выделим из признака даты: месяц, число, день недели. \n",
    "    #К дню недели прибавим 1, чтобы не было 0. В данных, где судебное решение не назначено, \n",
    "    #установим значение -1, чтобы модель могла более корректно отработать, 0 означает 0, а -1 - это значение\n",
    "    train['issued_day'] = train.ticket_issued_date.dt.day\n",
    "    train['issued_month'] = train.ticket_issued_date.dt.month\n",
    "    train['issued_weekday'] = train.ticket_issued_date.dt.weekday+1\n",
    "    train['hearing_day'] = train.hearing_date.dt.day\n",
    "    train['hearing_month'] = train.hearing_date.dt.month\n",
    "    train['hearing_weekday'] = train.hearing_date.dt.weekday+1\n",
    "\n",
    "    test['issued_day'] = test.ticket_issued_date.dt.day\n",
    "    test['issued_month'] = test.ticket_issued_date.dt.month\n",
    "    test['issued_weekday'] = test.ticket_issued_date.dt.weekday+1\n",
    "    test['hearing_day'] = test.hearing_date.dt.day\n",
    "    test['hearing_month'] = test.hearing_date.dt.month\n",
    "    test['hearing_weekday'] = test.hearing_date.dt.weekday+1\n",
    "\n",
    "    train = train.drop(['ticket_issued_date', 'hearing_date'], axis=1)\n",
    "    train = train.fillna(-1)\n",
    "\n",
    "    test = test.drop(['ticket_issued_date', 'hearing_date'], axis=1)\n",
    "    test = test.fillna(-1)\n",
    "    #--------------------------------------------------------------------\n",
    "    \n",
    "    #В признаке disposition мало уникальных значений, разложим через One Hot\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "    disposition_one_hot_train = train['disposition'].values.reshape(-1, 1)\n",
    "    disposition_one_hot_test = test['disposition'].values.reshape(-1, 1)\n",
    "\n",
    "    encoder.fit(disposition_one_hot_train)\n",
    "    encoder.fit(disposition_one_hot_test)\n",
    "\n",
    "    train_disposition = pd.DataFrame(encoder.transform(disposition_one_hot_train), \n",
    "                                 columns=encoder.categories_, \n",
    "                                 index=train.index)\n",
    "    train = train.drop('disposition', axis=1)\n",
    "\n",
    "    test_disposition = pd.DataFrame(encoder.transform(disposition_one_hot_test), \n",
    "                                columns=encoder.categories_, \n",
    "                                index=test.index)\n",
    "    test = test.drop('disposition', axis=1)\n",
    "    train = train.merge(train_disposition, left_index=True, right_index=True)\n",
    "    test = test.merge(test_disposition, left_index=True, right_index=True)\n",
    "    #-------------------------------------------------------------------------\n",
    "    #Предположим, что вероятность уплаты штрафа каким то образом может зависеть от района. \n",
    "    #Кластеризируем координаты адресов правонарушений\n",
    "    #Я хотел использовать DBSCAN, но у меня не хватает оперативной памяти на стационарном компьютере, \n",
    "    #чтобы провести данную кластеризацию, поэтому я проведу кластеризацию KMean, с разбивкой на 100 районов.\n",
    "    start_time_cluster = time.time()\n",
    "    print(\"    -----------------------------\")\n",
    "    print(\"    Начало кластеризации адресов...\")\n",
    "    latlon = pd.read_csv(r'C:\\datasets\\Сбербанк\\latlons.csv', low_memory=False)\n",
    "    addresses = pd.read_csv(r'C:\\datasets\\Сбербанк\\addresses.csv', low_memory=False)\n",
    "    \n",
    "    latlon = latlon.dropna()\n",
    "    lat_sc = latlon[['lat', 'lon']]\n",
    "    db = KMeans(n_clusters=100)\n",
    "    clusters = db.fit_predict(lat_sc)\n",
    "    latlon = latlon.merge(pd.DataFrame(clusters, index=latlon.index), left_index=True, right_index=True)\n",
    "    print(\"    Предобработка кластеризации завершена - %s seconds - OK\" % (time.time() - start_time_cluster))\n",
    "    print(\"    -----------------------------\")\n",
    "    \n",
    "    #Добавим результат кластеризации в обучающий и тестовый датасеты\n",
    "    total_ad = latlon.merge(addresses)\n",
    "    total_ad.set_index('ticket_id', inplace=True)\n",
    "    train = train.merge(total_ad, left_index=True, right_index=True)\n",
    "    test = test.merge(total_ad, left_index=True, right_index=True)\n",
    "    train = train.drop(['address', 'lat', 'lon'], axis=1)\n",
    "    test = test.drop(['address', 'lat', 'lon'], axis=1)\n",
    "    print(\"    Размер train после предобработки: {}\".format(train.shape))\n",
    "    print(\"    Размер test после предобработки: {}\".format(test.shape))\n",
    "    \n",
    "    #Перемешаем данные\n",
    "    train = shuffle(train)\n",
    "    \n",
    "    #Удалим дубликаты\n",
    "    train.drop_duplicates(inplace=True)\n",
    "    \n",
    "    print(\"Предобработка данных за - %s seconds - OK\" % (time.time() - start_time))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_model(train, test):\n",
    "    '''\n",
    "    Для предсказаний будем использовать модель классификации из библиотеки CatBoost\n",
    "    В черновике были проверены стандартные инструменты из sklearn(LogReg, SVC, RandomForest, GradientBoosting)\n",
    "    они показали себя хуже\n",
    "    \n",
    "    Так же подпор модели был сделан с помощью Pipeline и GridSearchCV, в финальном варианте не буду их использовать,\n",
    "    чтобы сэкономить процессорное время. Пользоваться данными инструментами умею.\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Обучение модели...\")\n",
    "    \n",
    "    #Загрузим модель\n",
    "    model = CatBoostClassifier(silent=True)\n",
    "    \n",
    "    #Разобъем train на признаки и целевую переменную\n",
    "    X = train.drop('compliance', axis=1)\n",
    "    y = train['compliance']\n",
    "    \n",
    "    #В наших данных имеется дисбаланс классов, \n",
    "    #устраним его адаптивным дополнением данных в обучающем наборе при помощи библиотеки imblearn\n",
    "    start_time_upsampling = time.time()\n",
    "    print(\"    Баланс классов до upsampling\")\n",
    "    print(\"    |Класс 0 :{:.2%} | Класс 1 :{:.2%}|\".format(train.compliance.value_counts(normalize=True)[0], \n",
    "                                           train.compliance.value_counts(normalize=True)[1]))\n",
    "    imbalance = ADASYN(random_state=42)\n",
    "    X_res, y_res = imbalance.fit_resample(X, y)\n",
    "    X_res, y_res = shuffle(X_res, y_res)\n",
    "    print(\"    upsampling завершен - %s seconds - OK\" % (time.time() - start_time_upsampling))   \n",
    "    \n",
    "    #Проверим точность модели на кросс-валидации\n",
    "    start_time_cv = time.time()\n",
    "    print(\"    -------------------------------\")\n",
    "    print(\"    Кросс-валидация началась...\")\n",
    "    print(\"    ROC AUC на кросс-валидации: {:.2f}\".format(cross_val_score(model, \n",
    "                                                                      X_res, y_res, \n",
    "                                                                      cv=5, \n",
    "                                                                      scoring='roc_auc')\n",
    "                                                      .mean()))\n",
    "    print(\"    Кросс-валидация завершена за - %s seconds - OK\\n\" % (time.time() - start_time_cv))\n",
    "    \n",
    "    print(\"    -------------------------------\")\n",
    "    start_time_fit = time.time()\n",
    "    print(\"    Обучение модели на полных данных...\")\n",
    "    model.fit(X_res, y_res)\n",
    "    print(\"    Обучение завершено за - %s seconds - OK\\n\" % (time.time() - start_time_fit))\n",
    "    \n",
    "    print(\"    -------------------------------\")\n",
    "    start_time_predict = time.time()\n",
    "    print(\"    Предсказание на тестовых данных...\")\n",
    "    test_predict = model.predict_proba(test)[:, 1]\n",
    "    print(\"    Предсказание завершено за - %s seconds - OK\\n\" % (time.time() - start_time_predict))\n",
    "    \n",
    "    test_predict = pd.DataFrame(data=[test.index, test_predict]).T\n",
    "    test_predict.columns = ['ticket_id', 'probablity']\n",
    "    test_predict.ticket_id = test_predict.ticket_id.astype('int64')\n",
    "    \n",
    "    print(\"Обучение завершено за - %s seconds - OK\" % (time.time() - start_time))\n",
    "    \n",
    "    return test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_predict():\n",
    "    train, test = loading_datasets()\n",
    "    train, test = preprocessing_datasets(train, test)\n",
    "    test_predict = learning_model(train, test)\n",
    "    test_predict.to_csv(r'C:\\datasets\\Сбербанк\\test_predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Загрузка данных началась\n",
      "Загрузка данных закончена за - 2.3001315593719482 seconds - OK\n",
      "-----------------------------\n",
      "Предобработка данных началась\n",
      "    Размер train до предобработки: (225000, 33)\n",
      "    Размер test до предобработки: (25305, 26)\n",
      "    -----------------------------\n",
      "    Начало кластеризации адресов...\n",
      "    Предобработка кластеризации завершена - 122.41600179672241 seconds - OK\n",
      "    -----------------------------\n",
      "    Размер train после предобработки: (144526, 21)\n",
      "    Размер test после предобработки: (25304, 20)\n",
      "Предобработка данных за - 128.6503586769104 seconds - OK\n",
      "-----------------------------\n",
      "Обучение модели...\n",
      "    Баланс классов до upsampling\n",
      "    |Класс 0 :90.44% | Класс 1 :9.56%|\n",
      "    upsampling завершен - 10.829619407653809 seconds - OK\n",
      "    -------------------------------\n",
      "    Кросс-валидация началась...\n",
      "    ROC AUC на кросс-валидации: 0.98\n",
      "    Кросс-валидация завершена за - 273.9536693096161 seconds - OK\n",
      "\n",
      "    -------------------------------\n",
      "    Обучение модели на полных данных...\n",
      "    Обучение завершено за - 61.935542345047 seconds - OK\n",
      "\n",
      "    -------------------------------\n",
      "    Предсказание на тестовых данных...\n",
      "    Предсказание завершено за - 0.016000986099243164 seconds - OK\n",
      "\n",
      "Обучение завершено за - 348.60093879699707 seconds - OK\n"
     ]
    }
   ],
   "source": [
    "run_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
